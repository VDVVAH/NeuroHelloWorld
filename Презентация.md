## Зачем нужна нейросеть?
Нейросеть способна эффективно справляться со следующими задачами:
1. Классификация - разделение обьектов на группы по признакам.
2. Кластеризация - разделение обьектов на множества максимально похожих.
3. Регрессия - адаптация под входные и выходные данные модели.
4. Прогнозирование - предсказание будущего по прошлому и настоящему.
5. Авторизация - нахождение коэффициента схожести обьектов и т.д.
6. Определение распределения функции вероятности.
7. Понижение размерности - изменение размерности и сложности входных данных.
8. Поиск ошибок, помех и т.п.
9. Ранжирование - сортировка данных по запросу.
10. Получение данных из большого их набора.

![](https://ejudge.179.ru/tasks/python/2022b/images/ai/ai5.png)

## Как работает нейросеть?
Нейросеть состоит из нейронов, которые являются математическими функциями.
Нейроны обьединяются в слои - совокупности независимых между собою нейронов.
В простейшей нейросети - *перцептроне*, есть один входной слой (в его нейроны поступают численные входные данные), несколько (т.е много) скрытых слоёв и один выходной слой (данные из его нейронов становятся результатом работы нейросети.)

#### Как могут обрабатываться данные перед входом в нейросеть?
1. Для изображений: сначало можно каждый пиксель отобразить в его HEX код(16-ричный RGB код), потом занести эти значения в промежутки от 0 до 1, т.е. можно разделить на 16^6 и подать на вход нейросети ввиде матрицы или вектора(массива).
2. Для букв, групп, слов, обьектов и т.п.: можно подобрать произвольный числовой код и также свести в промежуток от нуля до единицы.
3. Другие методы: главное чтобы числа были от 0 до 1(по крайней мере для перцептрона).

После можно действовать по разному в зависимости от того в какие промежутки попало значение выходных данных.
#### Как слои связаны между собою?
![Схема](https://proproprogs.ru/htm/neural_network/files/struktura-i-princip-raboty-polnosvyaznyh-neyronnyh-setey.files/image001.png "Схема")

От каждого нейрона предыдущего слоя информация в численном виде идёт к каждому нейрону следующего слоя (в перцептроне), как показано на рисунке выше.
#### Что из себя представляют математические функции в нейронах?
Нейрон можно рассматривать, как функцию правдоподобности какого-либо события по входным данным.

Функция правдоподобия, как следует из названия, показывает насколько правдоподобно событие по входным данным.
У каждого аргумента нейрона есть свой вес, обозначающий как сильно, положительно или отрицательно влияет на правдоподобность какого-то события данный аргумент. В простейших нейронах аргумент просто умножается на соответствующий ему вес. Потом сумму произведения аргументов на соответствующие им веса складывают с первоначальной вероятностью(также называется сдвигом или порогом **b**) события.

![](https://habrastorage.org/files/b64/d4c/138/b64d4c138872468bb77eb5b1d9badc1b.png)

Результат обычно подают на вход функции активации, которая заносит правдоподобность в какой либо промежуток (к примеру логистический сигмоид **σ**(x) имеет область значений (0; 1) и соответственно выход нейрона находится на этом промежутке).

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSjmpWSzKqaSAm0xziqZJfmJUP-KKCTmOWceA&s)

Сумма произведений входов на веса можно заменить скалярным произведением вектора весов на вектор входов, так получаем:
### y = σ(X*W + b)
Где **σ** - функция активации, **X** - вектор входов, **W** - вектор весов, **b** - сдвиг (первоначальная вероятность события).
#### Откуда берутся веса и сдвиг?
Веса и сдвиг находятся в процессе обучения нейросети.
Для обучения используется функция ошибки нейросети по входным данным.
В качестве функции ошибки можно использовать сумму квадратов отклонений результатов нейросети от идеальных результатов.
![](https://excel2.ru/sites/default/files/styles/mymedium_320/public/regress-50.png?itok=yl0Nu0Hv)

Задача обучения состоит в нахождении весов и сдвига, при которых функция ошибки на наибольшем количестве входных данных принимает наименьшее значение. Для этого в перцептроне используются разновидности градиентного спуска.

Иллюстрация метода градиентного спуска:

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4MmSaphHAXil0GUtUJfTSBkejf_hbF6wkxA&s)

При градиентном спуске от каждого веса и сдвига отнимается скорость роста ошибки нейросети умноженная на **шаг обучения** по аргументу, который соответствует этому весу (для сдвига (**b**) аргументом считается 1).

Шаг обучения в самых простых случаях является константой.

Скорость роста ошибки нейросети по аргументу, который соответствет весу, является производной от функции ошибки по этому аргументу (мы можем вычислить эту производную благодаря правилу диференцированния композиции функций (в нейронах)).

Т.е. получаем:

![](https://habrastorage.org/getpro/habr/upload_files/044/2cd/ff8/0442cdff8984295f9347a5e209900206.png)

В итоге нейросеть, обучившись, подстраивает веса и сдвиги под входные и выходные данные.

## Пример
Для примера я написал простой перцептрон, распознающий рукописные цифры.
Писал я его на python с использованием keras и всех зависимостей этой библиотеки, а также с локальным сервером на FastAPI.
За основу я взял: https://github.com/blaze-arch/MNIST-Classifier/blob/main/templates
Нейросеть может ошибаться, когда цифры расположены не также как в датасете, но в целом всё работает.
Для запуска лучше создать виртуальное окружение для python (python >= 3.10, т.к. я в коде использовал оператор match), установить в него библиотеки из requirements.txt, и в этом окружении запустить digit_recognition.py
![](https://github.com/VDVVAH/NeuroNetworkForHomework/blob/main/res/example1.png)

## Зачем понадобилось развитие нейросетей, если и перцептрон с указаными выше методами обучения работает?
1. Большой обьём данных для обучения. В современных нейросетях используются предварительные инициализации весов и сдвигов для оптимизации обучения нейросетей. Также используют другие более быстрые методы и адаптивные шаги обучения.

2. Производительность. Если в перцептроне получается очень много связей между нейронами, то это может серьёзно сказаться на производительности, тогда как в свёрточных нейросетях обьём данных с каждым слоем уменьшается вслед чего уменьшается число нейронов и их связей.

3. Специализация. Перцептрон воспринимает изображение как вектор входных числовых данных, в то время как в свёрточных сетях оно воспринимается как матрица, что повышает производительность, скорость обучения, качество и т.д.

4. Остальные факторы.

## Вывод
Нейросети с их адаптацией способны заменить практически что угодно, только производительность компьютеров могут этому помешать.
