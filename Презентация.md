## Зачем нужна нейросеть?
Нейросеть способна эффективно справляться со следующими задачами:
1. Классификация - разделение обьектов на группы по признакам.
2. Кластеризация - разделение обьектов на множества максимально похожих.
3. Регрессия - адаптация под входные и выходные данные модели.
4. Прогнозирование - предсказание будущего по прошлому и настоящему.
5. Авторизация - нахождение коэффициента схожести обьектов и т.д.
6. Определение распределения функции вероятности.
7. Понижение размерности - изменение размерности и сложности входных данных.
8. Поиск ошибок, помех и т.п.
9. Ранжирование - сортировка данных по запросу.
10. Получение данных из большого их набора.

![](https://ejudge.179.ru/tasks/python/2022b/images/ai/ai5.png)

## Как работает нейросеть?
Нейросеть состоит из нейронов, которые являются математическими функциями.
Нейроны обьединяются в слои - совокупности независимых между собою нейронов.
В простейшей нейросети - *перцептроне*, есть один входной слой (в его нейроны поступают численные входные данные), несколько (т.е много) скрытых слоёв и один выходной слой (данные из его нейронов становятся результатом работы нейросети).

#### Как могут обрабатываться данные перед входом в нейросеть?
1. Для изображений: сначало можно каждый пиксель отобразить в его HEX код(16-ричный RGB код), потом занести эти значения в промежуток от 0 до 1(к примеру разделить на 16^6(для цветного изображения)) и подать на вход нейросети ввиде матрицы или вектора(массива).
2. Для букв, групп, слов, обьектов и т.п.: можно подобрать произвольный числовой код и также свести в промежуток от нуля до единицы.
3. Другие методы: главное чтобы числа были от 0 до 1(по крайней мере для перцептрона).

После можно обрабатывать выход нейросети и запускать, к примеру, какие-нибудь методы в зависимости от того в какие промежутки попало значение выходных данных.
#### Как слои связаны между собою?
![Схема](https://proproprogs.ru/htm/neural_network/files/struktura-i-princip-raboty-polnosvyaznyh-neyronnyh-setey.files/image001.png "Схема")

От каждого нейрона предыдущего слоя информация в численном виде идёт к каждому нейрону следующего слоя (в перцептроне), как показано на рисунке выше.
#### Что из себя представляют математические функции в нейронах?
Нейрон можно рассматривать, как функцию правдоподобия какого-либо события по входным данным.

Функция правдоподобия, как следует из названия, показывает насколько правдоподобно событие по входным данным.
У каждого аргумента(входа) нейрона есть свой коэффициент(вес), обозначающий как сильно, положительно или отрицательно влияет на правдоподобность какого-то события данный аргумент. 
В простейших нейронах вход просто умножается на соответствующий ему вес. Потом сумму произведения аргументов на соответствующие им веса складывают с первоначальной вероятностью(также называется сдвигом или порогом **b**) события.

![](https://habrastorage.org/files/b64/d4c/138/b64d4c138872468bb77eb5b1d9badc1b.png)

Результат обычно подают на вход функции активации, которая заносит правдоподобность в какой либо промежуток (к примеру логистический сигмоид **σ**(x) имеет область значений (0; 1) и соответственно выход нейрона находится на этом промежутке).

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSjmpWSzKqaSAm0xziqZJfmJUP-KKCTmOWceA&s)

Сумму произведений входов на веса можно заменить скалярным произведением вектора весов на вектор входов, так получаем:

$y = σ(X^{T}W + b)$

Где **σ** - функция активации, **X** - вектор входов("T" вверху - это символ транспонирования вектора), **W** - вектор весов, **b** - сдвиг (первоначальная вероятность события).
#### Откуда берутся веса и сдвиг?
Веса и сдвиг находятся в процессе обучения нейросети.
Для обучения используется функция ошибки нейросети по входным данным.
В качестве функции ошибки можно использовать сумму квадратов отклонений результатов нейросети от идеальных результатов.
$$\displaystyle E(Y) = \sum_{i=1}^{n}(\hat{y_{i}}-y_{i})^{2}\.$$

Здесь $E(Y)$ - обозначение функции ошибки, 
$Y$ - вектор результатов, который выдала нейросеть по обучающим данным,
$\hat{Y}$ - вектор результатов, который должна была выдать эта нейросеть по таким же данным,
$y_{i} \in Y$, $\hat{y_{i}} \in \hat{Y_{i}}$,
$\Sigma_{i=1}^{n}$ - обозначение суммы по i от единицы до n, n - количество элементов вектора результатов.

Задача обучения состоит в нахождении весов и сдвига, при которых эта функция ошибки на наибольшем количестве входных данных принимает наименьшее значение. 
Для минимизации функции ошибки используются разновидности градиентного спуска.
Он основан на идеях матанализа, поэтому обьяснять я его не буду.

Иллюстрация метода градиентного спуска:

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4MmSaphHAXil0GUtUJfTSBkejf_hbF6wkxA&s)

## Пример
Для примера я написал простой перцептрон и свёрточную нейросеть, распознающий рукописные цифры.
Писал я его на python с использованием keras и всех зависимостей этой библиотеки, а также с локальным сервером на FastAPI.
За основу я взял: https://github.com/blaze-arch/MNIST-Classifier и https://habr.com/ru/articles/856426/
Перцептрон может ошибаться, когда цифры расположены не также как в датасете, но в целом всё работает, особенно свёрточный вариант.
Для запуска лучше создать виртуальное окружение для python (python >= 3.10, т.к. я в коде использовал оператор match), установить в него библиотеки из requirements.txt, и в этом окружении запустить digit_recognition_api.py
![](https://github.com/VDVVAH/NeuroNetworkForHomework/blob/main/res/example1.png)

## Зачем понадобилось развитие нейросетей, если и перцептрон с указаными выше методами обучения работает?
1. Большой обьём данных для обучения. В современных нейросетях используются предварительные инициализации весов и сдвигов для оптимизации обучения нейросетей. Также используют другие более быстрые методы и адаптивные шаги обучения.

2. Производительность. Если в перцептроне получается очень много связей между нейронами, то это может серьёзно сказаться на производительности, тогда как в свёрточных нейросетях обьём данных с каждым слоем уменьшается вслед чего уменьшается число нейронов и их связей.

3. Специализация. Перцептрон воспринимает изображение как вектор входных числовых данных, в то время как в свёрточных сетях оно воспринимается как матрица, что повышает производительность, скорость обучения, качество и т.д.

4. Остальные факторы.

## Вывод
Нейросети с их адаптацией способны заменить практически любой другой алгоритм, адаптировавшись под его входы и выходы, только производительность компьютеров могут этому помешать.
