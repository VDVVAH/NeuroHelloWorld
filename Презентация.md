## Как работает нейросеть?
Нейросеть состоит из нейронов, которые являются математическими функциями.
Нейроны обьединяются в слои - совокупности независимых между собою нейронов.
В простейшей нейросети - *перцептроне*, есть один входной слой (в его нейроны поступают численные входные данные), несколько (т.е много) скрытых слоёв и один выходной слой (данные из его нейронов становятся результатом работы нейросети.)
#### Как слои связаны между собою?
![Схема](https://proproprogs.ru/htm/neural_network/files/struktura-i-princip-raboty-polnosvyaznyh-neyronnyh-setey.files/image001.png "Схема")

От каждого нейрона предыдущего слоя информация в численном виде идёт к каждому нейрону следующего слоя (в перцептроне), как показано на рисунке выше.
#### Что из себя представляют математические функции в нейронах?
Нейрон можно рассматривать, как функцию правдоподобности какого-либо события по входным данным.
У каждого аргумента нейрона есть свой вес, обозначающий как сильно, положительно или отрицательно влияет на правдоподобность какого-то события данный аргумент. В простейших нейронах аргумент просто умножается на соответствующий ему вес. Потом сумму произведения аргументов на соответствующие им веса складывают с первоначальной вероятностью (**b**) события. Результат обычно подают на вход функции активации, которая заносит правдоподобность в какой либо промежуток (к примеру логистический сигмоид **σ**(x) имеет область значений (0; 1) и соответственно выход нейрона находится на этом промежутке).

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSjmpWSzKqaSAm0xziqZJfmJUP-KKCTmOWceA&s)

Сумма произведений входов на веса можно заменить скалярным произведением вектора весов на вектор входов, так получаем:
### y = σ(X*W + b)
Где **σ** - функция активации, **X** - вектор входов, **W** - вектор весов, **b** - сдвиг (первоначальная вероятность события).
#### Откуда беруться веса и сдвиг?
Веса и сдвиг находятся в процессе обучения нейросети.
Для обучения используется функция ошибки нейросети по входным данным.
В качестве функции ошибки можно использовать сумму квадратов отклонений результатов нейросети от идеальных результатов.
![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAS0AAACoCAMAAACCN0gDAAAAhFBMVEX///8AAADU1NT5+fnAwMBdXV38/PygoKDn5+erq6tkZGTx8fH19fX09PTt7e2ioqLf39/MzMyIiIhNTU24uLhCQkLi4uJ2dnazs7McHBwuLi6UlJQ3NzcnJyepqanFxcV3d3eMjIxXV1dubm4VFRU/Pz8pKSmAgIAPDw9JSUlBQUEaGhoNofklAAAJjUlEQVR4nO2d6WKyOhCGjaxqVRSrKO7aql/v//4OSUQDkg3CAdt5/tRWxDAkM+9MEtrpAAAAAAAAAAAAAAAAAAAAAAAAAL8fO/Y8L/kZhctV3HRjWk/gLRHqjNYIs2+6Ne1ng/p+YqhZnJhr0HRjWg9Cp6uDzXRAyG+6MW0nSLrUlLwaIzRquDGtx076Fn11Qhur2ba0n2T8BfTVFwqbbcob0EfIJS8+zmjfaEvegRVa0xdzhHrNNqX9fCC0oK8SBzZpti3tp4vQlr5K9Naw2ba0n+jRoxIH1mxT3oDpQ8AjdGy2KW/ADX3TkOimIhXgs0QOfZE4sKjZprSfRDZc6KseQo43b7Y1bWe03d6V/GTb23rdZlsDAAAAAECbcbs6BE03t2FGSIdL081tmImWtf56QjTWstZfF/j7pynOHod48XU/5KPp5ppjsIt6vWind0VTpufY/MNGDj5g41ZsYmvwHxft6JQPrDNjLsFxA/x+v2ojW8McLePJR8ffJpe/0/hcxFhLsLJmcEW/qUbop5NavqZ/WTPm4i9+IF13Vq2JLWKezjN3LgiNNT7YZax14h41+10hcThOXfBMdNUFnFQkwuGXhcQHE013PGCs9cU76FMSBd4WW7NvdTzGXDzXhN+TrCRp4zgdSvWBp+e3Eq5Pay05kgrn1MIFcPOQ2y8bxBfFecJJe2GfraYiRCR+7Vbuk7WShPKlcEmHVUJFHhlzlVlwOk9So89WzqlFEpnolVgzOldSEVywvDjof8wEQ0E1jhwwSm7kF3dZqC/M93hcVFQED7wEeqv/nUZwEJ/7DQzRY81jHndVquEW8yW6/udQ3ttVJxRYa38/5ptrrlA3Ht7pKaiIYrD+aC7dHtlcHpdB6sMF3sn6KjMMCWy6qPO5SdnI8D+Cu8L6RRoNNuVXQLLposa4IkO4XHf+H1mj1704AVpW2EHRZ8ylvrXgpNsXG2GHW5ntSDYKq9Q2fcZayioi0OyKTbHEY5H9w7bq/FXMmEtVRTh6PbExSCrMuPQp6gdPYVamk7lLbRVBVO07bNsgDWUW02alRqlKFFt0VgutpDt6r38fhw6GaZ5/xL/XIPi7N/pVzGg4kb9kNyqtsyrCgLU6/54n4NUismw4o3Z8pJMjz9Rxrhs+VOn2qfZh7hmpxeV2kyyyx2SVWTl3z6qIgh7zAvHxnIhIpnUZnfuxyt5dgwT5wEQmSbMpDRk3n2a/d8GYS2E3BkkAlsXvkUtgR942+f1aT616lYt4RLxnJ77oCgazKnrIWGshP5zcQ07WY+UDgM0/tipO7u6S7876LVpON1zeZYvO8uVHxGPwdAvOd9k9oYeX+20ML2eJbsHNRq+jszqbp7XkygDlRxvLJeenEut9m2njC7ucJcaZAENBojtb6YvvyFTEQBgOZtlT+AonTMbQIIvafmUr5zm+CrKRjcBrlIctOksOpSGUt+8zlxRNlULSFmVRicwdaopVpl0vwZfIinX+r1Vh0kXZ2raduAfiNx/6dKTkCDNlNoxidYPE8kfAuxWlriQSXNVOp8FjjZK0+EOnirie+8Z2z4Xa40tKWouIu9TN24Wqpq8yWvS5F51/5EJyJjbqAb97F+94WKr4oJLW6rIu4buwv5/qsRY1wk1BR1JrcTWMzdgyVIzeJf0W0Yn3gOcVR/NFTdbCoV8peEis5T8v11Z1sEM/i+r2bqzmaRAZchzkvhZruTgoqtUKJNZycaSikfyn7tVMl4cpLpzZ1lr61gjfJcWF3hK/RcQIieteqTldHYi/w10K1zoKSwp1WCsQRrkckphIExKLDo6a66uk3bji4fAEYA0xEV//WbmqQvN6/uxjOnewr790/3F38zuuBiaqm1MvKQfuDF/qVQ1a3uI/w4VEqi0+rP6l4ziHPxJvz3GQRMubLHApB8M7LrGWoNtQNx++6J/gZLzkfCE9p8cvNJFJB4UqlCIW7qt6a7yJtQQfwYrw3+TlWRyuctRVh/hQ3Nl5mkPWVj0GK6Rd/wkl9wuP7J/Ni/6xkPkYSer+a4GclfhYPbo6wTCFpJSCybR7nf+lqhSjq/GdjvSrVry3qY819K0kGGqfy5ZEZVoAu74miDU8Zu8oFn+kqVczwQbn8Cv9Ej+t7gg+txQHTZOQqU3+w4qI9nOMfNOi7JlIUUZQEcXJmfEKXDFE3PE1sGPKbZHMsFzFulfolrLv5/WPPd72tuazRl8YaK0fJIiXGpBHDStWRvIQCS3oPFFB8OvGq1JLYyWMEsfLd4ddQ2orkAwmMVNxpDkXejWnjrRxLxxopJ3Vl6jb1QKrJRyK4+JOu0I/xoOiL14XhDTTlGJwqFhVudGkIs6psw6LZxBHdSxicoT3vCcOAYrg7OooESFzz/MEGv/GTygWxSluUMPz2G1hVuOeTQiZo4rr88R6fcQdyhPOySPzEiwxh2gJ1dTAOBxtlILhSVxnIOHmu6CpFk87T80/e7YvPCWWYv8qfkP3qqbX8GHC0rPNatvBKj025OWdoalNtKe0v3jCsY2V2LpiWCHBUEEkBvLjJkxNIUj9R5/rSYSppQ7peXpCzecnt7vSYu8O9UYblShxEAS9R4MSvRlSYTWj8W70yc2lBqbytQEV5+5UGO92pROVJzgYhirjgah16SJUd59K3MS4/W2EfR1vK+zEVFEucZhfcTTdiASje6mive/nUAqGhL5i7r7b0MD52AHHjUE9U49aeqwCWPJFeuKzThWzQzJnqJIZDm26ynmvcKwbkyF4v4Az3yALU0W5dJHsRdDz/Wvl8IvT6HPfkXF8bCtTK0CTexj0DtNpvBNcgLGQOJrF0+khEjpfq3L9j925qYZJeYQd4SF6n/9/E2hby+S1WbHnxfH7/GuESG6eHE23uEliuXmycCdS/gJHuX2y7JtucZPoGusddmvWh6XLr3laIAAAAAAAAAAAAAAAzTHsh8IpmAH8d2EGPGHIf9eNORvY/ihHkbVICR+s9STgL7TZLdEJrKVIvIqsDlhLEbLkCaylA1jrSfdyiOP0CRWu/4oF1noynC2eS5OKZvw/r2AtBmaBcZG1bmAtlgOzgqZgKhH8VoabZGcVWItF9vQBsBaDL1tUCtZi6MoWGIO1GMay7RhgLYYLs8/CP/ZfOIC1GG7MA+pAncpgH9M42PZesKFvPRlJ9+2BtZ7Y0mWSYK0nnuTZNS62Vg2PUXlPHISOU94Wq62Tbl1ZH//0Qt0UO5pFPZ7gmkRRNMMkP2t61DkAAAAAAAAAAAAAvAX/AWvHYKi0E7lzAAAAAElFTkSuQmCC)
Задача обучения состоит в нахождении весов и сдвига, при которых функция ошибки на наибольшем количестве входных данных принимает наименьшее значение. Для этого в перцептроне используются разновидности градиентного спуска.
При градиентном спуске от каждого веса и сдвига отнимается скорость роста ошибки нейросети умноженная на **шаг обучения** по аргументу, который соответствует этому весу (для сдвига (**b**) аргументом считается 1).
Шаг обучения в самых простых случаях является константой.
Скорость роста ошибки нейросети по аргументу, который соответствет весу, является производной от функции ошибки по этому аргументу (мы можем вычислить эту производную благодаря правилу диференцированния композиции функций (наших нейронов),  также нам помогает то что это производная вычисляется в конкретной точке).

Т.е. получаем:

![](https://habrastorage.org/getpro/habr/upload_files/044/2cd/ff8/0442cdff8984295f9347a5e209900206.png)

В итоге нейросеть, обучившись, подстраивает веса и сдвиги под входные и выходные данные.
## Зачем понадобилось развитие нейросетей, если и перцептрон с указаными выше методами обучения работает?
1. Большой обьём данных для обучения. В современных нейросетях используются предварительные инициализации весов и сдвигов для оптимизации обучения нейросетей. Также используют другие более быстрые методы и адаптивные шаги обучения.

2. Производительность. Если в перцептроне получается очень много связей между нейронами, то это может серьёзно сказаться на производительности, тогда как в свёрточных нейросетях обьём данных с каждым слоем уменьшается вслед чего уменьшается число нейронов и их связей.

3. Специализация. Перцептрон воспринимает изображение как вектор входных числовых данных, в то время как в свёрточных сетях оно воспринимается как матрица, что повышает производительность, скорость обучения, качество и т.д.

4. Остальные факторы.

## Вывод
Нейросети с их адаптацией способны заменить практически что угодно, только производительность компьютеров могут этому помешать.
